<H1>Website scan</H1>

**Summery:** 

- [Nmap](#nmap)
- [Scan for cms](#scan-for-cms)
- [Scan directory](#scan-directory)
  - [Dico list](#dico-list)
  - [Commands](#commands)
- [HeartBleed](#heartbleed)
  - [Proxy](#proxy)
- [Crawler](#crawler)
  - [Wfuzz](#wfuzz)
  - [Gobuster](#gobuster)
  - [Useful Args](#useful-args)
- [Zaproxy](#zaproxy)
- [Check multiple page](#check-multiple-page)

## Nmap 

    nmap -p 80 -vv --script=http-enum.nse,http-methods.nse,http-majordomo2-dir-traversal.nse,http-auth.nse,http-passwd.nse,http-php-version.nse,http-phpmyadmin-dir-traversal.nse,http-put.nse,http-apache-negotiation.nse,http-adobe-coldfusion-apsa1301.nse IP_ADDRESS

## Scan for cms
> Cmsmap
> 
> `cmsmap http://<ipAddress:Port>`

> Wpscan
>
> `wpscan --url http://<ipAddress:Port>
    --disable-tls-checks`                    // if https
> Nikto

> `nikto -h http://<ipAddress:Port>`

## Scan directory 

### Dico list

    /usr/share/seclist/Discovery/Web-Content/common.txt

    usr/share/dirbuster/wordlists/common.txt
    usr/share/dirbuster/wordlists/directory-list-2.3-medium.txt
    usr/share/dirbuster/wordlists/directory-list-2.3-big.txt

    /usr/share/skipfish/dictionaries/complete.wl
### Commands

> Skipfish
> 
> `skipfish -m 5 -LY -S /usr/share/skipfish/dictionaries/complete.wl -u http://<ipAddress:Port>`

> Dirsearch
> 
> `dirsearch -u http://<ipAddress:Port> -e php,asp` 


## HeartBleed

    sslscan --targets=<ipAddress>


### Proxy 
    Burp

## Crawler 
### Wfuzz
    wfuzz -c -z file,'/usr/share/wfuzz/wordlist/Injections/All_attack.txt' http://<ipAddress:Port>/index.php?page=FUZZ

> If site in https 

    ffuf -w /usr/share/seclist/Discovery/Web-Content/common.txt -u https://target/FUZZ

### Gobuster
E.g.

    gobuster -w /usr/share/seclist/Discovery/Web-Content/common.txt -u http://<ipAddress:Port> 
> On request GET

    gobuster -e -u http//<ip>/ip?= -w /usr/share/wordlists/dirbuster/directory-list-2.3-medium.txt -t 10 -x .php,.txt,.html -s "200,301,302,307,403"
    
### Useful Args 

| Command             | Decryption                        |
|:--------------------|:----------------------------------|
| -a [useragent]      | User Agent                        |
| -t 10               | number threads                    |
| -r                  | Follow redirects                  |
| -c 'session=123456' | Set cookies                       |
| -k                  | Skip SSL certificate verification |
## Zaproxy
    zaproxy

## Check multiple page 

The file hosts has multiple different url :
    www.test.com www.google.com ...
    
    cat hosts |aquatone 

